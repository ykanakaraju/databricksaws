 
 Agenda - Databricks Platform Management
 ---------------------------------------
  Getting Started with AWS Databricks
  Databricks Workspace Components
  Notebooks Basics
  Databricks Utilities
  Unity Catalog (UC)
  Databricks Jobs & Workflows
  Data Governance and Security
  Delta Sharing & Lakehouse Federation
  Databricks Local Developement & Connect
  Deployment - Databricks Asset Bundles

  Github: https://github.com/ykanakaraju/databricksaws


  Getting Started
  ---------------

  1. Create an AWS account (free tier account is fine)
	https://aws.amazon.com/free

	Use any valid email address and complete the signup process. 


  2. Create Databricks Account

	Signup: https://login.databricks.com/signup

	Use a valid email and verify your email
	-> Choose the professional version
	-> Choose AWS cloud 
	and complete signup process. 


  3. Login to your AWS Databricks account console

	https://accounts.cloud.databricks.com/login


  Creating a Databricks Workspace
  -------------------------------

	1. Login to your AWS account.

	2. Login the Databricks account console.
		https://accounts.cloud.databricks.com/login

	3. Create a Workspace
		- Select "Workspaces" left menu option
		- Click on "Create Workspace" button
		- Create workspace as follows:
			Workspace name: <some name>
			Region: <some region. ex: 'us-east-1'>
			Storage and compute: Use your existing cloud account
			Click on 'Continue' button
		- Specify Cloud resources:
			Cloud credentials: Add cloud credentials
			Cloud storage: Add new cloud storage
			Click on 'Create workspace' button

	4. Approve the request to grant temporary token for Databricks
		- Scroll down and click on approve button.

	5. This will start provisioning the cloud resources for your workspace. 
		- This may take 10 to 15 minutes. 
		- Wait until the workspace is running.

	6. Click on 'Workspaces' left menu again (and refresh this page if required)
		Click on "Open" link of your workspace to open the workspace UI	
			

  Databricks Workspace Components
  -------------------------------

  1. Compute
	-> Serverless
		Compute resources are automatically provisioned and scaled as required. 
		You can not create serverless compute.

	-> All-purpose compute
		This is an interactive cluster.
		You can create the cluster with required configuration and use it as long as you need.
		The cost of the cluster is given in DBU/hour

	-> Job Compute
		This is a non-interactive cluster
		You can not create job-compute like All-purpose compute
		Job compute is a cluster configured to run a job.

	
 Assignments for Practice
 ------------------------

  1. Create AWS Databricks account. 
     Login to the Account Console and create a Databricks workspace

  2. Create an all-purpose single-node cluster without photon acceleration.
     Stop the cluster and edit the cluster configuration to enable photon acceleration.
     Terminate and delete th cluster.

  3. List out the differences between All-purpose and Job clusters?
     What is the purpose of Cluster pools?

  4. Create a catalog called "dev" and create a schema called "demodb" in the catalog.
     Create a table using UI by uploading "Baby_Names.csv" file (provided in the Github)
     Query the table in SQL window using 'SQL starter warehouse'

  5. Create a managed volume inside the "dev" catalog and upload some files and folders.
     In which S3 bucket are these files uploaded? 

     

   
	

   









































