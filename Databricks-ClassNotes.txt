 
 Agenda - Databricks Platform Management
 ---------------------------------------
  Getting Started with AWS Databricks
  Databricks Workspace Components
  Notebooks Basics
  Databricks Utilities
  Unity Catalog (UC)
  Databricks Jobs & Workflows
  Data Governance and Security
  Delta Sharing & Lakehouse Federation
  Databricks Local Developement & Connect
  Deployment - Databricks Asset Bundles

 -------------------------------------------------------
  Github Repo: 
  https://github.com/ykanakaraju/databricksaws
 -------------------------------------------------------


  Getting Started
  ---------------

  1. Create an AWS account (free tier account is fine)
	https://aws.amazon.com/free

	Use any valid email address and complete the signup process. 


  2. Create Databricks Account

	Signup: https://login.databricks.com/signup

	Use a valid email and verify your email
	-> Choose the professional version
	-> Choose AWS cloud 
	and complete signup process. 


  3. Login to your AWS Databricks account console

	https://accounts.cloud.databricks.com/login


  Creating a Databricks Workspace
  -------------------------------

	1. Login to your AWS account.

	2. Login the Databricks account console.
		https://accounts.cloud.databricks.com/login

	3. Create a Workspace
		- Select "Workspaces" left menu option
		- Click on "Create Workspace" button
		- Create workspace as follows:
			Workspace name: <some name>
			Region: <some region. ex: 'us-east-1'>
			Storage and compute: Use your existing cloud account
			Click on 'Continue' button
		- Specify Cloud resources:
			Cloud credentials: Add cloud credentials
			Cloud storage: Add new cloud storage
			Click on 'Create workspace' button

	4. Approve the request to grant temporary token for Databricks
		- Scroll down and click on approve button.

	5. This will start provisioning the cloud resources for your workspace. 
		- This may take 10 to 15 minutes. 
		- Wait until the workspace is running.

	6. Click on 'Workspaces' left menu again (and refresh this page if required)
		Click on "Open" link of your workspace to open the workspace UI	
			

  Databricks Workspace Components
  -------------------------------

  1. Compute
	-> Serverless
		Compute resources are automatically provisioned and scaled as required. 
		You can not create serverless compute.

	-> All-purpose compute
		This is an interactive cluster.
		You can create the cluster with required configuration and use it as long as you need.
		The cost of the cluster is given in DBU/hour

	-> Job Compute
		This is a non-interactive cluster
		You can not create job-compute like All-purpose compute
		Job compute is a cluster configured to run a job.


  Enabling DBFS file brower
  -------------------------
	
	Account icon (top-right) -> Setting -> Advanced -> Other -> Enable 'DBFS File Browser'
	Refresh the browse page to save the changes.


 Databricks 'dbutils'
 --------------------

	'help' command
	------------
	dbutils.help()
	dbutils.fs.help()
	dbutils.fs.help('ls')


	'ls' command
	------------
	dbutils.fs.ls("/")
	dbutils.fs.ls("/FileStore")
		list paths => [ d[0] for d in dbutils.fs.ls("/FileStore")]
	dbutils.fs.ls("/FileStore/testdata/csv")


	'mkdirs' command
	----------------
	dbutils.fs.mkdirs("/FileStore/testdata2")
	dbutils.fs.ls("/FileStore")


	'cp' command to copy files between DBFS directories
	---------------------------------------------------
	dbutils.fs.cp("/FileStore/testdata/csv/2011_summary.csv", "/FileStore/testdata2")
	dbutils.fs.cp("/FileStore/testdata/csv/2012_summary.csv", "/FileStore/testdata2")
	dbutils.fs.cp("/FileStore/testdata/csv/2013_summary.csv", "/FileStore/testdata2")

	dbutils.fs.ls("/FileStore/testdata/csv")


	'mv' command for moving or renaming files
	------------------------------------------
	dbutils.fs.help('mv')

	dbutils.fs.mv("/FileStore/testdata/csv", "/FileStore/testdata3", recurse=True)
	dbutils.fs.ls('/FileStore/testdata3')

	dbutils.fs.mv('/FileStore/testdata3/2011_summary.csv', '/FileStore/testdata3/2011_summary_renamed.csv')


	'rm' command to remove files & directories from DBFS
	-----------------------------------------------------
	dbutils.fs.rm('/FileStore/testdata3/2011_summary_renamed.csv')
	dbutils.fs.rm('/FileStore/testdata3/', recurse=True)


  Creating Stroage Credential
  ---------------------------
   https://docs.databricks.com/aws/en/connect/unity-catalog/cloud-storage/s3/s3-external-location-manual


 Assignments for Practice
 ------------------------

  1. Create AWS Databricks account.      
     Login to the Account Console and create a Databricks workspace.
     Make a note of all the compute and storage resources created.

  2. Create an all-purpose single-node cluster without photon acceleration.
     Stop the cluster and edit the cluster configuration to enable photon acceleration.
     Terminate and delete the cluster.

  3. List out the differences between All-purpose and Job clusters?
     What is the purpose of Cluster pools?

  4. Create a catalog called "dev" and create a schema called "demodb" in the catalog.
     Create a table using UI by uploading "Baby_Names.csv" file (provided in the Github repo)
     Query the table in SQL window using 'SQL starter warehouse'

  5. Create a managed volume inside the "dev" catalog and upload some files and folders.
     In which S3 bucket are these files uploaded? 

  6. Create a Workspace folder called "DatabricksPractice". 
     Create another folder inside it "Notebook basics"
     Create a notebook in the above folder and demo all basic notebook features including %run command

  7. Download you notebook into your local machine in different formats.
     Download the "AWSDatabricks.dbc" file from github and import it into your workspace,

  8. Create a notebook to demo file system operations using %fs 
     Use dbutils.fs utility package to copy only CSV files from one folder to another.

  9. Define the following bound parameters for a notebook using widgets utility.
	bound parameters: empid (text), city (dropdown) and skills (multi-select)
     Run this notebook from another notebook using dbutils.notebook utility.

  10. Create a catalog called 'dev' in your UC-workspace.
      Create a schema called 'demodb' in the catalog and create a table based on an uploaded file.

  11. What are the various data objects that you can have in a Unity-catalog metastore? List them
      Which object represents the IAM role required to connect to an external s3-bucket. 
      If you want to read data from an s3 bucket which object do you need to create in your workspace?

  12. Create a delta table called 'people' in the schema that you created earlier (i.e 'demodb')
      Perform a few insert, update and delete operations.
      List out all the versions and select the data of the delta table at different versions.
      Restore your table to an older version.
      Delete all unused data files that are not required in the current version. 

  13. Create a 'Storage Credential' and an 'External Location' to connect an S3 bucket.
      (You may create a sample S3 bucket in your AWS account and load some data into it for testing)

  14. What is your understanding of medallion architecture? 
      In whch which layer of the medallion architecture you create tables containing aggregated data?

  15. Execute the "UC-Movie-Data-Analysis" acse-study shared with you and create all the delta tables in bronze, silver and gold tables. 

   16. What your understanding of Unity Catalog Data access control and security model.
	Where do you control the grants for a specific catalog/schema/tables inside UC workspace.

   17. How do you examine all the notebooks, queries, tables etc in which a specific table is used (i.e lineage)







     

   
	

   






























